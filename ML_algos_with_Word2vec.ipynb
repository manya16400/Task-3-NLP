{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML algos with Word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lp_xaOZKwe9_",
        "outputId": "227e7e4f-62ed-4663-b159-31ccbe3d2aa8"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8426lV96rJ2o"
      },
      "source": [
        "### Word2vec function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4UFRfG_xE48"
      },
      "source": [
        "word2vec_path = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\n",
        "\n",
        "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
        "\n",
        "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
        "    if len(tokens_list)<1:\n",
        "        return np.zeros(k)\n",
        "    if generate_missing:\n",
        "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
        "    else:\n",
        "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
        "    length = len(vectorized)\n",
        "    summed = np.sum(vectorized, axis=0)\n",
        "    averaged = np.divide(summed, length)\n",
        "    return averaged\n",
        "\n",
        "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
        "    embeddings = clean_comments.apply(lambda x: get_average_word2vec(x, vectors, \n",
        "                                                                                generate_missing=generate_missing))\n",
        "    return list(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgcdt66rKKHn",
        "outputId": "64b6a31a-f046-4457-8874-0a772d334149"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/midas/train.csv')\n",
        "data.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'product_name', 'product_category_tree', 'description',\n",
              "       'brand', 'product_specifications', 'Label', 'Label_1st_category',\n",
              "       'des_preprocess', 'all_features_preprocess'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH6hV8cqKbrX"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Input, Flatten, Dropout, Add\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from keras.layers import LSTM, Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping\n",
        "import gensim\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import codecs\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPSm0MaPKh2o"
      },
      "source": [
        "# 1st category of product category tree as label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51U6lU-MKjRN"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l8gminYKpy1"
      },
      "source": [
        "categories = list(set(data['Label_1st_category']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGO-qG0IKtnz"
      },
      "source": [
        "# Linear SVM\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "def linear_svm(X_train, X_test, y_train, y_test):\n",
        "  \n",
        "    sgd =  SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
        "    sgd.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = sgd.predict(X_test)\n",
        " \n",
        "    return accuracy_score(y_pred, y_test), classification_report(y_test, y_pred,target_names=categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S701il2YKvT2"
      },
      "source": [
        "# Logistic Regrassion\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "def logisticreg(X_train, X_test, y_train, y_test):\n",
        "\n",
        "    logreg = LogisticRegression(n_jobs=1, C=1e5,max_iter=500)\n",
        "    logreg.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = logreg.predict(X_test)\n",
        "\n",
        "    return accuracy_score(y_pred, y_test), classification_report(y_test, y_pred,target_names=categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--2Og0iOKxMS"
      },
      "source": [
        "# Random Forest \n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "def randomforest(X_train, X_test, y_train, y_test):\n",
        "    \n",
        "    ranfor = RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
        "    ranfor.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = ranfor.predict(X_test)\n",
        "\n",
        "    return accuracy_score(y_pred, y_test), classification_report(y_test, y_pred,target_names=categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBN3UMl4Ky-2"
      },
      "source": [
        "# MLP CLassifier\n",
        "\n",
        "def mlpclassifier(X_train, X_test, y_train, y_test):\n",
        "  \n",
        "    from sklearn.neural_network import MLPClassifier\n",
        "    \n",
        "    \n",
        "    mlp = MLPClassifier(hidden_layer_sizes=(30,30,30))\n",
        "    mlp.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = mlp.predict(X_test)\n",
        "\n",
        "    return accuracy_score(y_pred, y_test), classification_report(y_test, y_pred,target_names=categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8XJJ72lLNlW",
        "outputId": "4670a813-c313-4abb-ca13-4a86e9f97cad"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXOrcA-PK07N",
        "outputId": "ce16b63b-87dc-4b9b-b9fe-f5127a04048c"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"\\033[1mUsing Description as feature\\033[0m\")\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokenized_x = data['des_preprocess'].apply(tokenizer.tokenize)\n",
        "# delete Stop Words\n",
        "tokenized_x = tokenized_x.apply(lambda vec: [word for word in vec if word not in stop_words])\n",
        "   \n",
        "training_embeddings = get_word2vec_embeddings(word2vec, tokenized_x, generate_missing=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(training_embeddings,list(data['Label_1st_category']) , test_size=0.2, random_state = 42) \n",
        "\n",
        "print(\"Linear SVM: \")\n",
        "acc,mat=linear_svm(X_train, X_test, y_train, y_test)\n",
        "print('Accuracy: ',acc)\n",
        "print(mat)\n",
        "print('-'*79)\n",
        "\n",
        "print(\"Logistic Reg: \")\n",
        "acc,mat=logisticreg(X_train, X_test, y_train, y_test)\n",
        "print('Accuracy: ',acc)\n",
        "print(mat)\n",
        "print('-'*75)\n",
        "\n",
        "print(\"Random Forest: \")\n",
        "acc,mat=randomforest(X_train, X_test, y_train, y_test)\n",
        "print('Accuracy: ',acc)\n",
        "print(mat)\n",
        "print('-'*75)\n",
        "\n",
        "print(\"MLP Classifier: \")\n",
        "acc,mat=mlpclassifier(X_train, X_test, y_train, y_test)\n",
        "print('Accuracy: ',acc)\n",
        "print(mat)\n",
        "print('-'*75)\n",
        "\n",
        "\n",
        "print(\"\\033[1mUsing product_name, description, brand, product_specifications as feature \\033[0m\")\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokenized_x = data['all_features_preprocess'].apply(tokenizer.tokenize)\n",
        "# delete Stop Words\n",
        "tokenized_x = tokenized_x.apply(lambda vec: [word for word in vec if word not in stop_words])\n",
        "   \n",
        "training_embeddings = get_word2vec_embeddings(word2vec, tokenized_x, generate_missing=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(training_embeddings,list(data['Label_1st_category']) , test_size=0.2, random_state = 42) \n",
        "\n",
        "print(\"Linear SVM: \")\n",
        "acc,mat=linear_svm(X_train, X_test, y_train, y_test)\n",
        "print('Accuracy: ',acc)\n",
        "print(mat)\n",
        "print('-'*75)\n",
        "\n",
        "print(\"Logistic Reg: \")\n",
        "acc,mat=logisticreg(X_train, X_test, y_train, y_test)\n",
        "print('Accuracy: ',acc)\n",
        "print(mat)\n",
        "print('-'*75)\n",
        "\n",
        "print(\"Random Forest: \")\n",
        "acc,mat=randomforest(X_train, X_test, y_train, y_test)\n",
        "print('Accuracy: ',acc)\n",
        "print(mat)\n",
        "print('-'*75)\n",
        "\n",
        "print(\"MLP Classifier: \")\n",
        "acc,mat=mlpclassifier(X_train, X_test, y_train, y_test)\n",
        "print('Accuracy: ',acc)\n",
        "print(mat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mUsing Description as feature\u001b[0m\n",
            "Linear SVM: \n",
            "Accuracy:  0.9018422567645366\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "Home Decor & Festive Needs       0.89      0.95      0.92       187\n",
            "                 Furniture       0.81      0.44      0.57        77\n",
            "  Beauty and Personal Care       0.76      0.48      0.59        46\n",
            "     Bags, Wallets & Belts       0.87      0.89      0.88       132\n",
            "                 Jewellery       0.94      1.00      0.97      1114\n",
            "          Home Improvement       0.78      0.77      0.78       108\n",
            "         Pens & Stationery       0.84      0.95      0.89       197\n",
            "                  Footwear       1.00      0.77      0.87        22\n",
            "                 Baby Care       0.88      0.76      0.82       182\n",
            "          Sports & Fitness       0.90      0.92      0.91       113\n",
            "                  Clothing       0.00      0.00      0.00         3\n",
            "           Home Furnishing       0.91      0.99      0.95       635\n",
            "                Automotive       0.82      0.84      0.83        97\n",
            "          Tools & Hardware       0.92      0.89      0.90       222\n",
            "    Toys & School Supplies       0.92      0.17      0.29        65\n",
            "                 Computers       0.94      0.59      0.73        27\n",
            "          Kitchen & Dining       0.89      0.81      0.85        84\n",
            "     Mobiles & Accessories       0.67      0.57      0.62        61\n",
            "                   Watches       1.00      0.99      1.00       102\n",
            "\n",
            "                  accuracy                           0.90      3474\n",
            "                 macro avg       0.83      0.73      0.76      3474\n",
            "              weighted avg       0.90      0.90      0.89      3474\n",
            "\n",
            "-------------------------------------------------------------------------------\n",
            "Logistic Reg: \n",
            "Accuracy:  0.9314910765687968\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "Home Decor & Festive Needs       0.97      0.95      0.96       187\n",
            "                 Furniture       0.68      0.79      0.73        77\n",
            "  Beauty and Personal Care       0.73      0.72      0.73        46\n",
            "     Bags, Wallets & Belts       0.87      0.85      0.86       132\n",
            "                 Jewellery       0.98      0.98      0.98      1114\n",
            "          Home Improvement       0.82      0.86      0.84       108\n",
            "         Pens & Stationery       0.96      0.96      0.96       197\n",
            "                  Footwear       0.90      0.82      0.86        22\n",
            "                 Baby Care       0.89      0.85      0.87       182\n",
            "          Sports & Fitness       0.92      0.96      0.94       113\n",
            "                  Clothing       0.50      0.33      0.40         3\n",
            "           Home Furnishing       0.98      0.98      0.98       635\n",
            "                Automotive       0.84      0.87      0.85        97\n",
            "          Tools & Hardware       0.93      0.95      0.94       222\n",
            "    Toys & School Supplies       0.66      0.72      0.69        65\n",
            "                 Computers       0.78      0.52      0.62        27\n",
            "          Kitchen & Dining       0.98      0.94      0.96        84\n",
            "     Mobiles & Accessories       0.67      0.59      0.63        61\n",
            "                   Watches       0.99      1.00      1.00       102\n",
            "\n",
            "                  accuracy                           0.93      3474\n",
            "                 macro avg       0.84      0.82      0.83      3474\n",
            "              weighted avg       0.93      0.93      0.93      3474\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "Random Forest: \n",
            "Accuracy:  0.8978123200921129\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "Home Decor & Festive Needs       0.97      0.91      0.94       187\n",
            "                 Furniture       1.00      0.56      0.72        77\n",
            "  Beauty and Personal Care       0.86      0.41      0.56        46\n",
            "     Bags, Wallets & Belts       0.78      0.84      0.81       132\n",
            "                 Jewellery       0.91      1.00      0.95      1114\n",
            "          Home Improvement       0.95      0.81      0.88       108\n",
            "         Pens & Stationery       0.71      0.87      0.78       197\n",
            "                  Footwear       0.93      0.59      0.72        22\n",
            "                 Baby Care       0.78      0.81      0.79       182\n",
            "          Sports & Fitness       0.94      0.90      0.92       113\n",
            "                  Clothing       1.00      0.33      0.50         3\n",
            "           Home Furnishing       0.94      0.99      0.97       635\n",
            "                Automotive       0.91      0.79      0.85        97\n",
            "          Tools & Hardware       0.94      0.93      0.94       222\n",
            "    Toys & School Supplies       0.89      0.25      0.39        65\n",
            "                 Computers       1.00      0.30      0.46        27\n",
            "          Kitchen & Dining       1.00      0.76      0.86        84\n",
            "     Mobiles & Accessories       0.72      0.69      0.71        61\n",
            "                   Watches       0.99      0.97      0.98       102\n",
            "\n",
            "                  accuracy                           0.90      3474\n",
            "                 macro avg       0.91      0.72      0.77      3474\n",
            "              weighted avg       0.90      0.90      0.89      3474\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "MLP Classifier: \n",
            "Accuracy:  0.9317789291882557\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "Home Decor & Festive Needs       0.93      0.96      0.95       187\n",
            "                 Furniture       0.85      0.78      0.81        77\n",
            "  Beauty and Personal Care       0.79      0.65      0.71        46\n",
            "     Bags, Wallets & Belts       0.92      0.81      0.86       132\n",
            "                 Jewellery       0.99      0.98      0.99      1114\n",
            "          Home Improvement       0.87      0.89      0.88       108\n",
            "         Pens & Stationery       0.95      0.96      0.96       197\n",
            "                  Footwear       0.67      0.91      0.77        22\n",
            "                 Baby Care       0.86      0.85      0.85       182\n",
            "          Sports & Fitness       0.88      0.97      0.92       113\n",
            "                  Clothing       1.00      0.67      0.80         3\n",
            "           Home Furnishing       0.97      0.97      0.97       635\n",
            "                Automotive       0.82      0.85      0.83        97\n",
            "          Tools & Hardware       0.96      0.94      0.95       222\n",
            "    Toys & School Supplies       0.58      0.69      0.63        65\n",
            "                 Computers       0.77      0.85      0.81        27\n",
            "          Kitchen & Dining       0.96      0.89      0.93        84\n",
            "     Mobiles & Accessories       0.66      0.62      0.64        61\n",
            "                   Watches       0.97      0.99      0.98       102\n",
            "\n",
            "                  accuracy                           0.93      3474\n",
            "                 macro avg       0.86      0.86      0.86      3474\n",
            "              weighted avg       0.93      0.93      0.93      3474\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "\u001b[1mUsing product_name, description, brand, product_specifications as feature \u001b[0m\n",
            "Linear SVM: \n",
            "Accuracy:  0.902993667242372\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "Home Decor & Festive Needs       0.88      0.96      0.92       187\n",
            "                 Furniture       0.69      0.47      0.56        77\n",
            "  Beauty and Personal Care       0.86      0.52      0.65        46\n",
            "     Bags, Wallets & Belts       0.86      0.89      0.87       132\n",
            "                 Jewellery       0.93      1.00      0.96      1114\n",
            "          Home Improvement       0.74      0.80      0.77       108\n",
            "         Pens & Stationery       0.93      0.95      0.94       197\n",
            "                  Footwear       0.95      0.91      0.93        22\n",
            "                 Baby Care       0.88      0.75      0.81       182\n",
            "          Sports & Fitness       0.93      0.86      0.89       113\n",
            "                  Clothing       0.00      0.00      0.00         3\n",
            "           Home Furnishing       0.92      0.99      0.96       635\n",
            "                Automotive       0.84      0.80      0.82        97\n",
            "          Tools & Hardware       0.92      0.86      0.89       222\n",
            "    Toys & School Supplies       0.96      0.35      0.52        65\n",
            "                 Computers       1.00      0.48      0.65        27\n",
            "          Kitchen & Dining       0.90      0.82      0.86        84\n",
            "     Mobiles & Accessories       0.64      0.61      0.62        61\n",
            "                   Watches       0.99      0.99      0.99       102\n",
            "\n",
            "                  accuracy                           0.90      3474\n",
            "                 macro avg       0.83      0.74      0.77      3474\n",
            "              weighted avg       0.90      0.90      0.90      3474\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "Logistic Reg: \n",
            "Accuracy:  0.9455958549222798\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "Home Decor & Festive Needs       0.99      0.97      0.98       187\n",
            "                 Furniture       0.72      0.77      0.74        77\n",
            "  Beauty and Personal Care       0.86      0.78      0.82        46\n",
            "     Bags, Wallets & Belts       0.90      0.92      0.91       132\n",
            "                 Jewellery       0.99      0.99      0.99      1114\n",
            "          Home Improvement       0.92      0.88      0.90       108\n",
            "         Pens & Stationery       0.94      0.96      0.95       197\n",
            "                  Footwear       0.96      1.00      0.98        22\n",
            "                 Baby Care       0.92      0.84      0.88       182\n",
            "          Sports & Fitness       0.90      0.98      0.94       113\n",
            "                  Clothing       0.50      0.33      0.40         3\n",
            "           Home Furnishing       0.98      0.99      0.98       635\n",
            "                Automotive       0.81      0.85      0.83        97\n",
            "          Tools & Hardware       0.96      0.96      0.96       222\n",
            "    Toys & School Supplies       0.76      0.69      0.73        65\n",
            "                 Computers       0.96      0.89      0.92        27\n",
            "          Kitchen & Dining       0.97      0.93      0.95        84\n",
            "     Mobiles & Accessories       0.66      0.75      0.70        61\n",
            "                   Watches       1.00      1.00      1.00       102\n",
            "\n",
            "                  accuracy                           0.95      3474\n",
            "                 macro avg       0.88      0.87      0.87      3474\n",
            "              weighted avg       0.95      0.95      0.95      3474\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "Random Forest: \n",
            "Accuracy:  0.8903281519861831\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "Home Decor & Festive Needs       0.95      0.90      0.93       187\n",
            "                 Furniture       1.00      0.60      0.75        77\n",
            "  Beauty and Personal Care       0.83      0.33      0.47        46\n",
            "     Bags, Wallets & Belts       0.88      0.79      0.83       132\n",
            "                 Jewellery       0.90      1.00      0.94      1114\n",
            "          Home Improvement       0.88      0.88      0.88       108\n",
            "         Pens & Stationery       0.76      0.84      0.80       197\n",
            "                  Footwear       0.95      0.82      0.88        22\n",
            "                 Baby Care       0.79      0.78      0.79       182\n",
            "          Sports & Fitness       0.93      0.84      0.88       113\n",
            "                  Clothing       0.00      0.00      0.00         3\n",
            "           Home Furnishing       0.89      1.00      0.94       635\n",
            "                Automotive       0.90      0.73      0.81        97\n",
            "          Tools & Hardware       0.95      0.94      0.94       222\n",
            "    Toys & School Supplies       1.00      0.31      0.47        65\n",
            "                 Computers       1.00      0.19      0.31        27\n",
            "          Kitchen & Dining       1.00      0.76      0.86        84\n",
            "     Mobiles & Accessories       0.67      0.61      0.64        61\n",
            "                   Watches       0.98      0.95      0.97       102\n",
            "\n",
            "                  accuracy                           0.89      3474\n",
            "                 macro avg       0.86      0.70      0.74      3474\n",
            "              weighted avg       0.89      0.89      0.88      3474\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "MLP Classifier: \n",
            "Accuracy:  0.9358088658606794\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "Home Decor & Festive Needs       0.98      0.95      0.97       187\n",
            "                 Furniture       0.79      0.81      0.80        77\n",
            "  Beauty and Personal Care       0.83      0.76      0.80        46\n",
            "     Bags, Wallets & Belts       0.94      0.83      0.88       132\n",
            "                 Jewellery       0.99      0.99      0.99      1114\n",
            "          Home Improvement       0.90      0.89      0.89       108\n",
            "         Pens & Stationery       0.94      0.97      0.96       197\n",
            "                  Footwear       0.80      0.91      0.85        22\n",
            "                 Baby Care       0.87      0.87      0.87       182\n",
            "          Sports & Fitness       0.91      0.97      0.94       113\n",
            "                  Clothing       0.00      0.00      0.00         3\n",
            "           Home Furnishing       0.99      0.98      0.98       635\n",
            "                Automotive       0.85      0.84      0.84        97\n",
            "          Tools & Hardware       0.92      0.96      0.94       222\n",
            "    Toys & School Supplies       0.56      0.60      0.58        65\n",
            "                 Computers       0.72      0.78      0.75        27\n",
            "          Kitchen & Dining       0.95      0.90      0.93        84\n",
            "     Mobiles & Accessories       0.62      0.69      0.65        61\n",
            "                   Watches       0.97      0.99      0.98       102\n",
            "\n",
            "                  accuracy                           0.94      3474\n",
            "                 macro avg       0.82      0.83      0.82      3474\n",
            "              weighted avg       0.94      0.94      0.94      3474\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TINZPymTopW"
      },
      "source": [
        "# category based on most common categories in the dataset as label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxhKi3W4Ts2h"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhqY4DguUVGW"
      },
      "source": [
        "# Linear SVM\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "def linear_svm(X_train, X_test, y_train, y_test):\n",
        "  \n",
        "    sgd =  SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
        "    sgd.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = sgd.predict(X_test)\n",
        "    #print(len(set(y_pred)),len(set(y_test)),len(set(y_train)))\n",
        "    return accuracy_score(y_pred, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqyqvq_pUVGy"
      },
      "source": [
        "# Logistic Regrassion\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "def logisticreg(X_train, X_test, y_train, y_test):\n",
        "\n",
        "    logreg = LogisticRegression(n_jobs=1, C=1e5,max_iter=500)\n",
        "    logreg.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = logreg.predict(X_test)\n",
        "\n",
        "    return accuracy_score(y_pred, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAlp15AKUVG0"
      },
      "source": [
        "# MLP CLassifier\n",
        "\n",
        "def mlpclassifier(X_train, X_test, y_train, y_test):\n",
        "  \n",
        "    from sklearn.neural_network import MLPClassifier\n",
        "    \n",
        "    \n",
        "    mlp = MLPClassifier(hidden_layer_sizes=(30,30,30))\n",
        "    mlp.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = mlp.predict(X_test)\n",
        "\n",
        "    return accuracy_score(y_pred, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h95kAgEyTvfT",
        "outputId": "323e6ada-0902-4cad-d097-835dafb4c5aa"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"\\033[1mUsing Description as feature\\033[0m\")\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokenized_x = data['des_preprocess'].apply(tokenizer.tokenize)\n",
        "# delete Stop Words\n",
        "tokenized_x = tokenized_x.apply(lambda vec: [word for word in vec if word not in stop_words])\n",
        "   \n",
        "training_embeddings = get_word2vec_embeddings(word2vec, tokenized_x, generate_missing=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(training_embeddings,list(data['Label']) , test_size=0.2, random_state = 42) \n",
        "\n",
        "print(\"Linear SVM: \")\n",
        "acc=linear_svm(X_train, X_test, y_train, y_test)\n",
        "print('Accuracy: ',acc)\n",
        "\n",
        "\n",
        "print(\"Logistic Reg: \")\n",
        "acc=logisticreg(X_train, X_test, y_train, y_test)\n",
        "print('Accuracy: ',acc)\n",
        "\n",
        "\n",
        "print(\"MLP Classifier: \")\n",
        "acc=mlpclassifier(X_train, X_test, y_train, y_test)\n",
        "print('Accuracy: ',acc)\n",
        "\n",
        "print('-'*70)\n",
        "\n",
        "\n",
        "print(\"\\033[1mUsing product_name, description, brand, product_specifications as feature \\033[0m\")\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokenized_x = data['all_features_preprocess'].apply(tokenizer.tokenize)\n",
        "# delete Stop Words\n",
        "tokenized_x = tokenized_x.apply(lambda vec: [word for word in vec if word not in stop_words])\n",
        "   \n",
        "training_embeddings = get_word2vec_embeddings(word2vec, tokenized_x, generate_missing=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(training_embeddings,list(data['Label']) , test_size=0.2, random_state = 42) \n",
        "\n",
        "print(\"Linear SVM: \")\n",
        "acc=linear_svm(X_train, X_test, y_train, y_test)\n",
        "print('Accuracy: ',acc)\n",
        "\n",
        "\n",
        "print(\"Logistic Reg: \")\n",
        "acc=logisticreg(X_train, X_test, y_train, y_test)\n",
        "print('Accuracy: ',acc)\n",
        "\n",
        "\n",
        "print(\"MLP Classifier: \")\n",
        "acc=mlpclassifier(X_train, X_test, y_train, y_test)\n",
        "print('Accuracy: ',acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mUsing Description as feature\u001b[0m\n",
            "Linear SVM: \n",
            "Accuracy:  0.7748992515831894\n",
            "Logistic Reg: \n",
            "Accuracy:  0.902993667242372\n",
            "MLP Classifier: \n",
            "Accuracy:  0.8316062176165803\n",
            "----------------------------------------------------------------------\n",
            "\u001b[1mUsing product_name, description, brand, product_specifications as feature \u001b[0m\n",
            "Linear SVM: \n",
            "Accuracy:  0.7711571675302246\n",
            "Logistic Reg: \n",
            "Accuracy:  0.91335636154289\n",
            "MLP Classifier: \n",
            "Accuracy:  0.8347725964306275\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}